 The vehicle. I hope that it will be more effective. Stop. Thank you. Thank you. Thank you. That's how we do it. Thank you very much. Thank you. Yes. jesh pratiksha darvik Thank you. Thank you. What is happening? Yes. Thank you. That's all. Thanks a lot. Please go ahead. I know there's just two minutes. You really are going to be so talented, Marley. This is our business case and next followed up where we will discuss regarding the architecture, further ASR and summary models and we will conclude with the demo. and the theater school. So first let me get started with the business. As. We all know that all the sectors that we are going to cause. Oh, I hope it looks like there's more working. for the future. But I think, you know, one thing is very common. Those are tactic movies. All the meetings that we have, we need to have discussions. Decisions, action options. However, as soon as the meeting ends... The action items need to, action items tend to slip away. the actual business agenda always tends to fade away and the the valuable insights, we're going to focus. So, this will definitely lead to more time spend. direct-to-follow. and hamper the productivity of the meeting as well as the project. So further, we have a pictured solution, which will transform the cows into plants. That is which See you next time. productivity and it is definitely the need to find approach for collaboration and all. So I target what it would be, corporate teams, project leaders. a project management field, and remote work. So further we'll discuss about the technologies used in the project. So. We have used React Frontend, Flashbackend and Socket.io for transferring audio to the servers. and hugging fish transformers for like in summary models and all those machine learning. All right. What, uh, language? So next is the architecture, so what we have did is we built a web-based platform as in previous presentation we have discussed we are not going for a Uh Chrome extension. So it's better for us and then we have user socket IO for like real-time prescriptions. So as we speak, it will transcribe in real time. and I have used the Vojtech. activity detection. So, as I already. told in previous presentation we are getting some issues with the like chunking the audio one by one, so I have use this to tackle the problems where the words when person is speaking the audio should not be chunked in between the words. So it detects the void in my voice and then chunks the audio properly. And next is. We tried to, initially we tried to train our own ASR model but we did not get the good accuracy cause obvious reasons. So we use open ears whisper. So we directly did not use the open air whisper. There is a one wrap around the open air whisper which is called the faster whisper. which is a slightly faster version of the Whisper Coaster. We need a real-time transcriptions but So, since it's a faster or a smaller model, the accuracy is relatively lower side. And then... We use, we train our three models for Generating the minutes of the meeting And then at the end, after generating the minutes of the meeting, we are basically emailing it to the user. So it's a basic architecture like. So we have designed So when the user comes on the. website, it's basically signed up. I want to be threatened. And then, as soon as the user comes on to be protected, the web socket was initiated. If you can, you can start the recording. So as soon as we start recording and start talking So, audio just starts going on the server. And then we are going to try to... and give the description back. the Uh Like the transmission is generated at a center of content and that is where the Now let's get slide to the project. And. As the disease is going on... At the back end, we are setting the basket system. So whatever junk in the sentences are there. So, we are saving it in a text file so that it can be let us use for the generation of the merits of the people. So, as soon as we stop the recording and click on the generation I do enjoy it. We take the transcriptions and run it through our model. and create a Docswipe. for the minutes of the meeting and send. like in an attachment. Who knows? So, this is the overall architecture and next we have some models. So, as Boudin explained, so all this was mainly divided into two parts. So, first we had to convert the audio into text and then summarize it. So, totally we had four models and this is the first model that is the ISR model. So basically started from scratch. We had a CNN event in London, so first of all, we tried to find out the audio value to factor down. and try to have parents to dance. because not every Oreo had a safe land and The speech from one could have been... with major discrepancy. So we've tried, we've seen a long time. And then Uh, had a six layer of RNN. and for the evaluation we use word has a rate and character has a rate and we also have connection in standard classification loss so which is like Basically, if you if you have all your files then if someone has say other than the duration might be a too short when ah when . as you say, say too long. And you have to fill the, fill the spectrogram because then if the duration can't begin. So for that, there is a, this condition is tackled classification class we have to measure. and we didn't have a good accuracy on this particular model and the dataset that we had, basically the main dataset AML corpus had calculation of 30 minutes, helping and solving we had. So rather than truncating it in smaller chunks, we use as a fixed data set. We've had 30,000 audio gigs. And for one epoch, it was acquired 36 on the... If you would I didn't GPU for one year ago because it was 8 hours So we didn't put much effort into this model because we had the project was given in this one. So we'll be Jules. We prefer to convert the audio to text and we further move on for the summarization. the extent to the question. For the summarization task, we have used more than 160 transcripts with abstracts, so. It is having his seven to ten different different domains like with some software That's it from software companies and general discussions. as well as normal making, so... Uh huh. We have 7 to 10 different domains and for the General's not ready. Thank you. for the general summary generation we have used the auto model per sequence to sequence using the large flat. language model. We have a then almost around the teapot for and it will take out. All right. 30 to 40 hours to rain on the is what else. Uh-oh, I wanted to... The results are like this. The rocks flow is 0.7 into 0.7, and then off. And... and 2 is 0.02. I know that these scores are not that much too, because it... These folds are out. scores are, you know, like extracts and generated summary, so it is not a matching field. That much because transcripts are more and the summaries are less so Is this matching? word by word so. Full time, no? That must. Can you see the label? And. Instead of we can use the coolant simulation for the theories and magic. And the second modal is The second model is for the action item extraction. action items like Thank you so much. It means like, if somebody's saying like that... such as We will address this one. to have find it. Three different phases, so... The upgrade is the action word, so it will detect that word. And on the basis of that, it will classify for the class. Thank you. Thank you. Bye. classifications because this model is for the works on the context. of this sentence, like, if the There is a contact in the second, it will classify. that as an action. Uh What is like a... Um There is a remote company and they are discussing like We will improve the hardware on that. remote. So our intro and the remote. improve is the action bar and the remote is the context for that. So on that, that is equally ideal. If it has an action item and then I try to... request those sentences with you. T5 conditioners and data. And these are the... accuracy we got. 0.9 feet. So. on the and for the taste of the beef. Hello. They got a. Sorry, there is a title. This goes as the upper right side. And the hard one is that, some items, some items, some items, some items, some items. for conditional generated. Yeah, there you go, now, bye. conditioner, they have to shut down tears. generate Thanks. automatically like it made some context and here we had the classified sentences so we can give it to the model and it will generate the Thank you so much, somebody. So yeah. Evolution matter. evaluation scores for that, Timmy. Semantics are semantic similarity, which is potent similarity, I got to we got 0.63 and the diversity is 4, diversity is 4 is like Uh and the word in the summary. So it is saying 0.88, which means 88 percentage of the. Words are different. and automated VWC tags. index which is like Any student We study in the second side, it can return somebody's soul. it is more readable. And these are the... All right, scores. So. We've got Bye-bye. Thank you very much. Thank you very much. And... particle of 0.8 in. with... The. Which is not that much good? and We cannot consider that as a school because... And go back up to my point... conditional generation, generation is that. It is not having a all the words and all the sentences which are provided in the model and If one Don't use this, somebody has sent me a quote. Oh. So I can see so we can Because rely on these codes. Please show that more. Oh. at the table. If you can say that, some... That is really good. Everybody is having different differences. These were the models we had. Yep. It's like. Thank you. So, as you know, our end product's all full, and it's okay, thank you. Please leave your email to the user and I will see you in the next video. And that's the So this is how it's going to look. So we have the abstract of the summary. That's the whole meeting summary here on that. and later on we do have some few important key points. and then you have some action items like that only just explain what is action. and I can just let emphasis as well. own life. what has to be done for it. So we are going to make the summit look. Oh So this is how it's going to look, and this is the format that's going to be. And that's what it is. So. went So. The user has interacted, so let's go M1. So next we will go with the future scope. So we have an enhanced speaker, speaker recognition as of now in our meeting like all of us have thought. So it's going to just take as a whole. But in our future, we just want to make it for an individual person who said what. and the work that they're emphasizing on So that again, our a model can adapt to different kinds of scenarios like the multiple crossing details and everything. and get the... action items. And next we are going to optimize the ASR technology so that like we will redefine the summarization. And I thought... Um long paraglides, and also at the earl. then it's going to be there now, it's going to give a total summary. but we will enhance it with just at a glance when we just look at it at a Glands are just people get to know like what is the software. And then we have a target action items. As of now, it's just the audio. Next, we will try to integrate the... video formats too and we can even integrate like Zoom and Slack. wearing at our own comfort space so we can get the MOMs or during the meetings. so we don't have to come out of the platform at all. for the first time in my life. Again, get that. So this is going to be our future score. Love. with the user acceptance and the demo is initially on fire. Yeah, so to ensure the product like meet the user expectation so we right now what we have done of what we can do. like We have just tested the happy scenarios in our system like we did not touch the. HPSS and different accents and all that stuff, the accuracy of those things. And Like we did not really had tested this system into a real life meeting. where like useful is this with me. get to test. So, and user acceptance thing overall will be going to do this. like visually etch get is meeting is too long like let's say four hours going on and the number of text like the task is increasing If the meeting is too short, there is not enough content to generate. generate the somebody's. what will happen we are going to test all those things And. multiple accents what I slightly have done not like completely tested, but it is performing good, so why? But what I did is, I tried to find a podcast on the YouTube and type of blade. And, uh... put my system into this. and try to see how accurately it catches with the different accents. So, it is pretty good not that bad, but. We should look more into... this like to get the numbers like how accurate it is. with different accents. we get to. do that So, this is all. going to be done in the user acceptance. in the future. The next is demo. So as we are talking I have transcripted my whole presentation. Yeah? to we have tried to Uh So I'm trying to generate the minutes of the meeting for this presentation. So right now, whatever we have talked. uh... It has generated transcripts for that.